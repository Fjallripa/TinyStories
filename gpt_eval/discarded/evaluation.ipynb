{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator setup\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "evaluator_model = \"gpt-4-1106-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get question\n",
    "question_ind = np.random.choice(60)\n",
    "question_data = json.load(open(\"evaluation/eval_questions.json\", \"r\"))[question_ind]\n",
    "question = question_data[\"question\"]\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get context\n",
    "query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": question,\n",
    "            \"fields\": [\"title^2\", \"abstract\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "retriever_response = retriever_client.search(\n",
    "    body=query,\n",
    "    index=index_name\n",
    ")\n",
    "titles = []\n",
    "abstracts = []\n",
    "for i, hit in enumerate(retriever_response[\"hits\"][\"hits\"]):\n",
    "    titles.append(hit['_source']['title'])\n",
    "    abstracts.append(hit['_source']['abstract'])\n",
    "    if i == 2:  # only keep top 3 results\n",
    "        break\n",
    "context = f\"\"\"\n",
    "    Article 1: {titles[0]}\n",
    "    {abstracts[0]}\n",
    "    Article 2: {titles[1]}\n",
    "    {abstracts[1]}\n",
    "    Article 3: {titles[2]}\n",
    "    {abstracts[2]}\n",
    "\"\"\"\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get answer\n",
    "# model_prompt = f\"\"\"\n",
    "# Please answer the following question using the context provided.\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "# model_inputs = tokenizer(\n",
    "#     model_prompt,\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_attention_mask=False\n",
    "# )\n",
    "# model_outputs = model.generate(**model_inputs, max_length=500)\n",
    "# answer = tokenizer.batch_decode(model_outputs.flatten()[len(model_inputs[\"input_ids\"].flatten()):])[0]\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get answer from chatgpt-3.5 for testing\n",
    "model_prompt = f\"\"\"\n",
    "Please answer the following question using the context provided.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "for i in range(10):\n",
    "        try:\n",
    "                chat_completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": model_prompt}],\n",
    "                stream=False,\n",
    "                # max_tokens=100,\n",
    "                )\n",
    "                if isinstance(chat_completion, dict):\n",
    "                        # not stream\n",
    "                        answer = chat_completion.choices[0].message.content\n",
    "                else:\n",
    "                        # stream\n",
    "                        for token in chat_completion:\n",
    "                                answer = token[\"choices\"][0][\"delta\"].get(\"content\")\n",
    "                break\n",
    "        except Exception as exc:\n",
    "                print(traceback.format_exc())\n",
    "                print(exc)\n",
    "                if i != 9:\n",
    "                        print(f\"Retrying... (i = {i})\")\n",
    "                        time.sleep(3)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evaluation\n",
    "eval_prompt = f\"\"\"\n",
    "        Your task is to evaluate a student's response to a given exercise. In the exercise, the student is provided with some general context consisting of 3 titles and abstracts of medical articles.\n",
    "        The student is furthermore asked a question, which he should answer correctly making use of the provided context.\n",
    "        The exercise tests the student's abilities regarding grammar, reading comprehension and logical reasoning. The student's answer starts after the *** symbol.\n",
    "        Please provide your general assessment about the answer provided by the student (the part after the *** symbol).\n",
    "        Is it correct? Is it grammatically correct? Is it consistent with the given context?\n",
    "        Furthermore, grade the studentâ€™s answer in terms of grammar, coherence, consistency with the context and whether it is correct or not. Moreover, please provide your best guess of what the academic degree of the student might be, as reflected from the answer. Choose from possible 4 possible categories: A: no degree. B: bachelor's degree. C: master's degree. D: doctoral degree. Use the following grade format: Grammar: #/10, Coherence: #/10, Context: #/10, Correctness: #/10, where the \"#\" should be replaces by a number between 0 (worst) and 10 (best).\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Answer: *** {answer}\n",
    "\"\"\"\n",
    "for i in range(10):\n",
    "        try:\n",
    "                chat_completion = openai.ChatCompletion.create(\n",
    "                model=evaluator_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "                stream=False,\n",
    "                # max_tokens=100,\n",
    "                )\n",
    "                if isinstance(chat_completion, dict):\n",
    "                        # not stream\n",
    "                        eval_output = chat_completion.choices[0].message.content\n",
    "                else:\n",
    "                        # stream\n",
    "                        for token in chat_completion:\n",
    "                                eval_output = token[\"choices\"][0][\"delta\"].get(\"content\")\n",
    "                break\n",
    "        except Exception as exc:\n",
    "                # print(traceback.format_exc())\n",
    "                # print(exc)\n",
    "                if i == 0:\n",
    "                        print(\"Retrying.\", end=\"\")\n",
    "                if i != 9:\n",
    "                        print(f\".\", end=\"\")\n",
    "                        time.sleep(3)\n",
    "print(\"\")\n",
    "print(eval_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
