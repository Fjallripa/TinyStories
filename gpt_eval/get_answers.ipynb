{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_core.vectorstores import VectorStoreRetriever, VectorStore\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = 9200\n",
    "auth = (\"admin\", \"admin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"gpt-3.5-turbo\"\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "\n",
    "if \"gpt\" in model_id:\n",
    "    hf = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_id,\n",
    "        token=hf_token\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_id,\n",
    "        token=hf_token,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    ) # device_map=\"cuda\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        pad_token_id=2,\n",
    "        max_length=1000\n",
    "        # max_new_tokens=800\n",
    "    )\n",
    "    hf = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs={'do_sample':True,\n",
    "        # \"max_new_tokens\":200,\n",
    "        \"temperature\":0.7,\n",
    "        \"top_k\":50,\n",
    "        'top_p':0.95,\n",
    "        'num_return_sequences':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"} # cuda:0\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedder = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore: VectorStore = OpenSearchVectorSearch(\n",
    "    \"https://localhost:9200\",\n",
    "    embedding_function=embedder,\n",
    "    index_name=\"pubmed_abstracts\",\n",
    "    vector_dim=384,\n",
    "    http_compress = True,\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    )\n",
    "\n",
    "# retriever: VectorStoreRetriever = vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.5})\n",
    "retriever: VectorStoreRetriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"min_score\": 0.4, 'k': 5}\n",
    ")\n",
    "# embeddings = HuggingFaceBgeEmbeddings()\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embedder)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embedder, similarity_threshold=0.76)\n",
    "# create a transformer for the document compressor that extracts the page_content from the documents\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "CONTEXT: {context}\n",
    "Answer the following query with the above given CONTEXT. Answer honestly and correctly using accurate scientific terminology, if you don't know the answer, you must say so. Do not create questions for yourself.\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SYSTEM_TEMPLATE,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(hf, question_answering_prompt)\n",
    "\n",
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "def debug_print(this):\n",
    "    print(type(this))\n",
    "    for t in this:\n",
    "        print(t, type(t))\n",
    "\n",
    "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_transforming_retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
    "        # If only one message, then we just pass that message's content to retriever\n",
    "        (lambda x: x[\"messages\"][-1].content) | compression_retriever,\n",
    "    ),\n",
    "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
    "    query_transform_prompt | hf | StrOutputParser() | compression_retriever,\n",
    ").with_config(run_name=\"chat_retriever_chain\")\n",
    "\n",
    "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=query_transforming_retriever_chain,\n",
    ").assign(\n",
    "    answer=document_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(context):\n",
    "    clean_context = []\n",
    "    for doc in context:\n",
    "        new_doc = {}\n",
    "        doc = dict(deepcopy(doc))\n",
    "        new_doc[\"page_content\"] = doc[\"page_content\"]\n",
    "        new_doc[\"metadata\"] = doc[\"metadata\"]\n",
    "        del new_doc[\"metadata\"][\"text_chunk_id\"]\n",
    "        del new_doc[\"metadata\"][\"text\"]\n",
    "        del new_doc[\"metadata\"][\"vector_field\"]\n",
    "        new_doc[\"similarity_score\"] = doc[\"state\"][\"query_similarity_score\"]\n",
    "        clean_context.append(new_doc)\n",
    "    return clean_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "answers = []\n",
    "for q in tqdm(questions):\n",
    "    for i in range(10):  # avoid model evading answer\n",
    "        a = conversational_retrieval_chain.invoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=q[\"question\"])\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        if not (\"I'm sorry\" in a[\"answer\"]):\n",
    "            break\n",
    "    answers.append({\n",
    "        \"id\": q[\"id\"],\n",
    "        \"type\": q[\"type\"],\n",
    "        \"question\": q[\"question\"],\n",
    "        \"context\": prepare_context(a[\"context\"]),\n",
    "        \"answer\": a[\"answer\"],\n",
    "    })\n",
    "with open(f\"eval_answers_{model_id}.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_answer(questions, id):\n",
    "    q = questions[id]\n",
    "    for i in range(10):  # avoid model evading answer\n",
    "        a = conversational_retrieval_chain.invoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=q[\"question\"])\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        if not (\"I'm sorry\" in a[\"answer\"]):\n",
    "            break\n",
    "    print(a)\n",
    "    print(a[\"context\"])\n",
    "    return {\n",
    "        \"id\": q[\"id\"],\n",
    "        \"type\": q[\"type\"],\n",
    "        \"question\": q[\"question\"],\n",
    "        \"context\": prepare_context(a[\"context\"]),\n",
    "        \"answer\": a[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in tqdm(range(60)):\n",
    "    with open(f\"eval_answers_{model_id}.json\", \"r\") as f:\n",
    "        eval_answers = json.load(f)\n",
    "    eval_answers[id] = get_single_answer(questions, id)\n",
    "    with open(f\"eval_answers_{model_id}.json\", \"w\") as f:\n",
    "        json.dump(eval_answers, f, indent=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 58\n",
    "with open(f\"eval_answers_{model_id}.json\", \"r\") as f:\n",
    "    eval_answers = json.load(f)\n",
    "eval_answers[id] = get_single_answer(questions, id)\n",
    "with open(f\"eval_answers_{model_id}.json\", \"w\") as f:\n",
    "    json.dump(eval_answers, f, indent=\"    \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
