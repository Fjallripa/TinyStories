{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"healio\"\n",
    "# model_id = \"gpt-3.5-turbo\"\n",
    "q_types = [\"yes_no\", \"factual\", \"list\", \"causal\", \"hypothetical\", \"complex\"]\n",
    "criteria = [\"Grammar\", \"Coherence\", \"Context\", \"Correctness\"]\n",
    "models = [\"healio\", \"gpt-3.5-turbo\"]\n",
    "scores = {}\n",
    "for model_id in models:\n",
    "    with open(f\"evaluations_{model_id}.json\", \"r\") as f:\n",
    "        evaluations = json.load(f)\n",
    "    scores[model_id] = {q_type: {\n",
    "        \"Grammar\": 0.0,\n",
    "        \"Coherence\": 0.0,\n",
    "        \"Context\": 0.0,\n",
    "        \"Correctness\": 0.0,\n",
    "    } for q_type in q_types}\n",
    "    for q in evaluations:\n",
    "        e = q[\"evaluation\"]\n",
    "        if \"/10\" in e:\n",
    "            e = e.strip().replace(\"\\n\", \"\").replace(\"/10\", \"/10 \").replace(\".\", \" \").replace(\":\", \" \").replace(\"(\", \" \").replace(\")\", \" \").split(\" \")\n",
    "            e = [w for w in e if w != \"\"]\n",
    "            for i, tok in enumerate(e):\n",
    "                if \"/10\" in tok:\n",
    "                    try:\n",
    "                        nom = float(tok.split(\"/\")[0])\n",
    "                        denom = float(tok.split(\"/\")[1])\n",
    "                        scores[model_id][q[\"type\"]][e[i-1]] += nom / denom\n",
    "                    except Exception as ex:\n",
    "                        continue  # unnecessary total grades\n",
    "        else:\n",
    "            pass\n",
    "            # print(\"GRADING FAILED!\")\n",
    "            # for k, v in q.items():\n",
    "            #     print(k, v)\n",
    "    # print(model_id)\n",
    "    # for k, v in scores.items():\n",
    "    #     print(f\"    {k} : {np.round(v/6, 2)}/10\")\n",
    "    # print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for crit in criteria:\n",
    "    print(crit)\n",
    "    headers = [\"q-type\"] + models\n",
    "    t = []\n",
    "    avg = {m: 0.0 for m in models}\n",
    "    for qt in q_types:\n",
    "        row = [qt]\n",
    "        for m in models:\n",
    "            row.append(np.round(scores[m][qt][crit], 2))\n",
    "            avg[m] += scores[m][qt][crit]\n",
    "        t.append(row)\n",
    "    t.append([\"average\"] + [np.round(avg[m]/6, 1) for m in models])\n",
    "    print(tabulate(t, headers=headers))\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
