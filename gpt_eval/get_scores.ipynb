{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gooog\\anaconda3\\envs\\nlp_clean\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "import traceback\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "if \"s\" not in openai.api_key:\n",
    "    print(\"KEY LOADING FAILED!\")\n",
    "    raise NameError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_api(model, eval_prompt1, eval_prompt2):\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            eval_step1 = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                stream=False,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": eval_prompt1}],\n",
    "            )\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            print(traceback.format_exc())\n",
    "            print(exc)\n",
    "            if i != 9:\n",
    "                print(f\"Retrying... (i = {i})\")\n",
    "            eval_step1 = {\"choices\": [{\"message\": {\"content\": \"ERROR IN STEP 1\"}}]}\n",
    "            time.sleep(3)\n",
    "    if \"ERROR\" in eval_step1[\"choices\"][0][\"message\"][\"content\"]:\n",
    "        eval_step2 = {\"choices\": [{\"message\": {\"content\": \"ERROR IN STEP 1\"}}]}\n",
    "        return eval_step1, eval_step2\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            eval_step2 = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                stream=False,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": eval_prompt1},\n",
    "                    {\"role\": \"assistant\", \"content\": eval_step1.choices[0].message.content},\n",
    "                    {\"role\": \"user\", \"content\": eval_prompt2},\n",
    "                ]\n",
    "            )\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            print(traceback.format_exc())\n",
    "            print(exc)\n",
    "            if i != 9:\n",
    "                print(f\"Retrying... (i = {i})\")\n",
    "            eval_step2 = {\"choices\": [{\"message\": {\"content\": \"ERROR IN STEP 2\"}}]}\n",
    "            time.sleep(3)\n",
    "    return eval_step1, eval_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_prompts(answer, lang):\n",
    "    if lang == \"en\":\n",
    "        eval_prompt1 = f\"\"\"In the following exercise, the student is given a beginning of a story. The student needs to complete it into a full story.\n",
    "The exercise tests the student's language abilities and creativity. The symbol *** marks the separator between the prescribed beginning and the student's completion:\n",
    "{answer[\"prompt\"]}***{answer[\"completion\"]}\n",
    "Please provide your general assessment about the part written by the student (the one after the *** symbol).\n",
    "Is it gramatically correct? Is it consistent with the beginning of the story? Pay special attention to whether the student manages to complete the sentence which is split in the middle by the separator ***.\"\"\"\n",
    "        eval_prompt2 = f\"\"\"Now, grade the student’s completion in terms of grammar, creativity, consistency with the story’s beginning and whether the plot makes sense.\n",
    "Moreover, please provide your best guess of what the age of the student might be, as reflected from the completion.\n",
    "Choose from possible age groups: A: 3 or under. B: 4-5. C: 6-7. D: 8-9. E: 10-12. F: 13-16.\n",
    "Use the format below replacing # and X with your answer and do not write anything more than that:\n",
    "grammar: #/10, creativity: #/10, consistency: #/10, age: X\"\"\"\n",
    "    else:\n",
    "        eval_prompt1 = f\"\"\"In der folgenden Übung bekommt der Schüler den Anfang einer Geschichte vorgegeben. Der Schüler muss sie zu einer vollständigen Geschichte ausarbeiten.\n",
    "Die Übung testet die Sprachfähigkeiten und die Kreativität des Schülers. Das Symbol *** markiert die Trennung zwischen dem vorgegebenen Anfang und dem vom Schüler vervollständigten Teil:\n",
    "{answer[\"prompt\"]}***{answer[\"completion\"]}\n",
    "Bitte geben Sie Ihre allgemeine Beurteilung über den vom Schüler geschriebenen Teil (den Teil nach dem *** Symbol) ab.\n",
    "Ist er grammatikalisch korrekt? Ist er konsistent mit dem Anfang der Geschichte? Achten Sie besonders darauf, ob der Schüler den Satz vervollständigt, der in der Mitte durch das Trennzeichen *** geteilt wird.\"\"\"\n",
    "        eval_prompt2 = f\"\"\"Jetzt bewerten Sie bitte den vom Schüler vervollständigten Teil in Bezug auf Grammatik, Kreativität, Konsistenz mit dem Anfang der Geschichte und ob die Handlung Sinn ergibt.\n",
    "Darüber hinaus geben Sie bitte Ihre beste Einschätzung ab, wie alt der Schüler sein könnte, basierend auf der Vervollständigung.\n",
    "Wählen Sie aus möglichen Altersgruppen: A: 3 oder jünger. B: 4-5. C: 6-7. D: 8-9. E: 10-12. F: 13-16.\n",
    "Benutzen Sie das folgende Format, wobei Sie # und X mit Ihrer Antwort ersetzen, und sonst nichts weiteres schreiben.\n",
    "Grammatik: #/10, Kreativität: #/10, Konsistenz: #/10, Alter: X\"\"\"\n",
    "    return eval_prompt1, eval_prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_eval(eval_model, answer, lang):\n",
    "    eval_prompt1, eval_prompt2 = get_eval_prompts(answer, lang)\n",
    "    eval_step1, eval_step2 = prompt_api(eval_model, eval_prompt1, eval_prompt2)\n",
    "    answer[\"eval_step1\"] = eval_step1[\"choices\"][0][\"message\"][\"content\"]\n",
    "    answer[\"eval_step2\"] = eval_step2[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evals(eval_model, lang, model_id):\n",
    "    with open(f\"answers_{lang}_{model_id}.json\", \"r\") as f:\n",
    "        answers = json.load(f)\n",
    "\n",
    "    evaluations = []\n",
    "    for a in tqdm(answers):\n",
    "        try:\n",
    "            ev = get_single_eval(eval_model, a, lang)\n",
    "        except Exception as e:\n",
    "            print(\"error in get_single_eval\")\n",
    "            ev = a\n",
    "            ev[\"eval_step1\"] = \"ERROR: SOMETHING WENT WRONG\"\n",
    "            ev[\"eval_step2\"] = \"ERROR: SOMETHING WENT WRONG\"\n",
    "        evaluations.append(ev)\n",
    "    with open(f\"evaluations_{lang}_{model_id}.json\", \"w\") as f:\n",
    "        json.dump(evaluations, f, indent=\"    \", ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating english own_small-model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [38:18<00:00, 22.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german own_small-model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [58:34<00:00, 35.15s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_model = \"gpt-4-0125-preview\"\n",
    "lang = \"en\"\n",
    "# for model_id in [\"gpt\", \"ts\", \"own_small\"]:\n",
    "for model_id in [\"own_small\"]:\n",
    "    print(f\"Evaluating english {model_id}-model.\")\n",
    "    get_evals(eval_model, lang, model_id)\n",
    "\n",
    "lang = \"de\"\n",
    "# for model_id in [\"gpt\", \"own_small\", \"own_big\"]:\n",
    "for model_id in [\"own_small\"]:\n",
    "    print(f\"Evaluating german {model_id}-model.\")\n",
    "    get_evals(eval_model, lang, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
