{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "import traceback\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_api(model, eval_prompt):\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            chat_completion = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "            stream=False,\n",
    "            # max_tokens=100,\n",
    "            )\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            print(traceback.format_exc())\n",
    "            print(exc)\n",
    "            if i != 9:\n",
    "                print(f\"Retrying... (i = {i})\")\n",
    "            chat_completion = {\"choices\": [{\"message\": {\"content\": \"ERROR\"}}]}\n",
    "            time.sleep(3)\n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_prompt(answer_doc):\n",
    "    context = \"\"\"\"\"\"\n",
    "    for i, article in enumerate(answer_doc[\"context\"]):\n",
    "        context += f\"   Article {i+1}: {article['metadata']['title']}\\n\"\n",
    "        context += f\"   Abstract: {article['metadata']['abstract']}\\n\"\n",
    "    question = f\"\"\"{answer_doc[\"question\"]}\"\"\"\n",
    "    answer = f\"\"\"{answer_doc[\"answer\"]}\"\"\"\n",
    "    eval_prompt = f\"\"\"Your task is to evaluate a student's response to a given exercise. In the exercise, the student is provided with some general context consisting of the titles and abstracts of medical articles.\n",
    "The student is furthermore asked a question, which he should answer correctly making use of the provided context.\n",
    "The exercise tests the student's abilities regarding grammar, reading comprehension and logical reasoning. The student's answer starts after the *** symbol.\n",
    "Please provide your general critical assessment about the answer provided by the student (the part after the *** symbol).\n",
    "Is it correct? Is it grammatically correct? Is it consistent with the given context?\n",
    "Furthermore, grade the studentâ€™s answer in terms of grammar, coherence, consistency with the context and whether it is correct or not. Use the following grade format: Grammar: #/10, Coherence: #/10, Context: #/10, Correctness: #/10, where the \"#\" should be replaces by a number between 0 (worst) and 10 (best).\n",
    "Remember to be very strict about the grading!\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: *** {answer}\"\"\"\n",
    "    return eval_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-4-1106-preview\"\n",
    "# model_id = \"gpt-3.5-turbo\"\n",
    "model_id = \"healio\"\n",
    "\n",
    "with open(f\"eval_answers_{model_id}.json\", \"r\") as f:\n",
    "    answers = json.load(f)\n",
    "\n",
    "evaluations = []\n",
    "for a in tqdm(answers):\n",
    "    evals = []\n",
    "    for i in range(10):\n",
    "        eval_prompt = get_eval_prompt(a)\n",
    "        chat_completion = prompt_api(eval_model, eval_prompt)\n",
    "        evals.append(chat_completion[\"choices\"][0][\"message\"][\"content\"])\n",
    "    evaluations.append({\n",
    "        \"id\": a[\"id\"],\n",
    "        \"type\": a[\"type\"],\n",
    "        \"question\": a[\"question\"],\n",
    "        \"context\": a[\"context\"],\n",
    "        \"answer\": a[\"answer\"],\n",
    "        \"evaluation\": evals\n",
    "    })\n",
    "with open(f\"evaluations_{model_id}_repeated.json\", \"w\") as f:\n",
    "    json.dump(evaluations, f, indent=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-4-1106-preview\"\n",
    "model_id = \"gpt-3.5-turbo\"\n",
    "# model_id = \"healio\"\n",
    "\n",
    "with open(f\"eval_answers_{model_id}.json\", \"r\") as f:\n",
    "    answers = json.load(f)\n",
    "\n",
    "evaluations = []\n",
    "for a in tqdm(answers):\n",
    "    evals = []\n",
    "    for i in range(10):\n",
    "        eval_prompt = get_eval_prompt(a)\n",
    "        chat_completion = prompt_api(eval_model, eval_prompt)\n",
    "        evals.append(chat_completion[\"choices\"][0][\"message\"][\"content\"])\n",
    "    evaluations.append({\n",
    "        \"id\": a[\"id\"],\n",
    "        \"type\": a[\"type\"],\n",
    "        \"question\": a[\"question\"],\n",
    "        \"context\": a[\"context\"],\n",
    "        \"answer\": a[\"answer\"],\n",
    "        \"evaluation\": evals\n",
    "    })\n",
    "with open(f\"evaluations_{model_id}_repeated.json\", \"w\") as f:\n",
    "    json.dump(evaluations, f, indent=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_score(eval_answers, id):\n",
    "    a = eval_answers[id]\n",
    "    eval_prompt = get_eval_prompt(a)\n",
    "    chat_completion = prompt_api(eval_model, eval_prompt)\n",
    "    return {\n",
    "        \"id\": a[\"id\"],\n",
    "        \"type\": a[\"type\"],\n",
    "        \"question\": a[\"question\"],\n",
    "        \"context\": a[\"context\"],\n",
    "        \"answer\": a[\"answer\"],\n",
    "        \"evaluation\": chat_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in tqdm(range(60)):\n",
    "    with open(f\"evaluations_{model_id}.json\", \"r\") as f:\n",
    "        evaluations = json.load(f)\n",
    "    evaluations[id] = get_single_score(answers, id)\n",
    "    with open(f\"evaluations_{model_id}.json\", \"w\") as f:\n",
    "        json.dump(evaluations, f, indent=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
