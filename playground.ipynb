{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pretrained\n",
    "# model_id = \"roneneldan/TinyStories-1M\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# prompt = \"Once upon a time there was\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# output = model.generate(input_ids, max_length=100, num_beams=1)\n",
    "# output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load untrained\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "config = AutoConfig.from_pretrained(model_id, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "prompt = \"Once upon a time there was\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=100, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_carr_ret(poem):\n",
    "    return poem.replace(\"\\r\", \"\")\n",
    "poems = pd.read_csv(\"data/PoetryFoundationData.csv\")[\"Poem\"].apply(clean_carr_ret)\n",
    "poems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = min(max([len(tokenizer.encode(p)) for p in tqdm(poems)]), 2048)\n",
    "print(f\"{max_length = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in tqdm(txt_list):\n",
    "            encodings_dict = tokenizer(\n",
    "                \"<|startoftext|>\" + txt + \"<|endoftext|>\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "            self.input_ids.append(torch.tensor(encodings_dict[\"input_ids\"]))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict[\"attention_mask\"]))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PoemDataset(poems, tokenizer, max_length=max_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    if len(d) > 10:\n",
    "        print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 10,\n",
    "    logging_steps=2,\n",
    "    save_steps = 4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.05,\n",
    "    logging_dir = \"./logs\",\n",
    "    report_to = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = lambda data: {\n",
    "        \"input_ids\": torch.stack([f[0] for f in data]),\n",
    "        \"attention_mask\": torch.stack([f[1] for f in data]),\n",
    "        \"labels\": torch.stack([f[0] for f in data])\n",
    "    }\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model.pth\")\n",
    "generated = tokenizer(\"<|startoftext|>\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_outputs = model.generate(\n",
    "    generated,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    max_length=50,\n",
    "    top_p=0.95,\n",
    "    temperature=1,\n",
    "    num_return_sequences=2000,\n",
    ")\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Helsinki-NLP/opus-mt-en-de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gooog\\anaconda3\\envs\\nlp_clean\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gooog\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "encoding\n",
      "translating\n",
      "decoding\n",
      "\n",
      "Einmal gab es ein kleines Auto namens Beep. Beep liebte es, schnell zu gehen und in der Sonne zu spielen. Beep war ein gesundes Auto, weil er immer guten Treibstoff hatte. Guter Treibstoff machte Beep glücklich und stark. Eines Tages fuhr Beep im Park, als er einen großen Baum sah. Der Baum hatte viele Blätter, die fielen. Beep mochte, wie die Blätter fallen und mit ihnen spielen wollte. Beep fuhr unter dem Baum und beobachtete, wie die Blätter auf ihn fielen. Er lachte und piepste sein Horn. Beep spielte mit den fallenden Blättern den ganzen Tag. Als es Zeit war, nach Hause zu gehen, wusste Beep, dass er mehr Treibstoff brauchte. Er ging zum Kraftstoffplatz und bekam mehr gesunden Treibstoff. Nun war Beep bereit, schnell zu gehen und am nächsten Tag wieder zu spielen. Und Beep lebte glücklich bis ans Ende.\n"
     ]
    }
   ],
   "source": [
    "prompts = \"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong. One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn. Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.\"\n",
    "# prompts = [s + \".\" for s in story.split(\". \")][:-1]\n",
    "# for p in prompts:\n",
    "#     print(p)\n",
    "print(\"\")\n",
    "print(\"encoding\")\n",
    "encodings = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "# print(encodings)\n",
    "print(\"translating\")\n",
    "trans_codes = model.generate(**encodings)\n",
    "# print(trans_codes)\n",
    "print(\"decoding\")\n",
    "translations = [tokenizer.decode(t, skip_special_tokens=True) for t in trans_codes]\n",
    "print(\"\")\n",
    "for t in translations:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
